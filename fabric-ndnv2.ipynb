{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85eefb5e-bbe7-4aa2-8366-ef4e33e4098a",
   "metadata": {},
   "source": [
    "## NDN-DPDK with python-ndn across FABRIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c603af-8a4a-4a08-9900-2114506b558b",
   "metadata": {},
   "source": [
    "this notebook/experiment is hosted at https://github.com/jthak002/ndn-fabricv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72c16a-43df-4c57-aa4b-e20cdf8a9732",
   "metadata": {},
   "source": [
    "Setup incase you do not have your fabric environment setup properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646396e-82de-46e9-96e6-74550b50e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source ~/work/fabric_config/fabric_rc\n",
    "printenv | grep FABRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca65569-64c5-43ee-bed3-d3583a955cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify your project ID\n",
    "os.environ['FABRIC_PROJECT_ID']=''\n",
    "\n",
    "# Set your Bastion username and private key\n",
    "os.environ['FABRIC_BASTION_USERNAME']=''\n",
    "os.environ['FABRIC_BASTION_KEY_LOCATION']=os.environ['HOME']+'/work/.ssh/id_rsa'\n",
    "os.environ['FABRIC_JUPYTER_USERNAME'] = ''\n",
    "# You can leave the rest on the default settings\n",
    "# Set the keypair FABRIC will install in your slice. \n",
    "os.environ['FABRIC_SLICE_PRIVATE_KEY_FILE']=os.environ['HOME']+'/work/fabric_config/slice_key'\n",
    "os.environ['FABRIC_SLICE_PUBLIC_KEY_FILE']=os.environ['HOME']+'/work/fabric_config/slice_key.pub'\n",
    "# Bastion IPs\n",
    "os.environ['FABRIC_BASTION_HOST'] = 'bastion.fabric-testbed.net'\n",
    "\n",
    "# make sure the bastion key exists in that location!\n",
    "# this cell should print True\n",
    "os.path.exists(os.environ['FABRIC_BASTION_KEY_LOCATION'])\n",
    "\n",
    "# prepare to share these with Bash so we can write the SSH config file\n",
    "FABRIC_BASTION_USERNAME = os.environ['FABRIC_BASTION_USERNAME']\n",
    "FABRIC_BASTION_KEY_LOCATION = os.environ['FABRIC_BASTION_KEY_LOCATION']\n",
    "FABRIC_SLICE_PRIVATE_KEY_FILE = os.environ['FABRIC_SLICE_PRIVATE_KEY_FILE']\n",
    "FABRIC_BASTION_HOST = os.environ['FABRIC_BASTION_HOST']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156a2b6-5593-437e-b2e4-2ab9eaf32459",
   "metadata": {},
   "source": [
    "## Setting up the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889065f0-0764-4607-9a8a-21db036e040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager()\n",
    "fablib.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6ba15-31f9-4d8e-b932-fad41978e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "SLICENAME=\"<YOUR-SLICENAME-HERE>\"\n",
    "SLICE_EXISTS = False\n",
    "try:\n",
    "    fabric_slice = fablib.get_slice(SLICENAME)\n",
    "    print(\"You already have a slice named %s.\" % SLICENAME)\n",
    "    SLICE_EXISTS = True\n",
    "    print(slice)\n",
    "except:\n",
    "    fabric_slice = fablib.new_slice(name=SLICENAME)\n",
    "    print(\"You will need to create a %s slice.\" % SLICENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8da33-961f-4bef-ba41-303cfc686648",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_needed= { 'node1' : {'cpu': 8, 'ram': 80, 'nic': 1, 'disk': 60}, 'node2' : {'cpu': 8, 'ram': 80, 'nic': 1, 'disk': 60}}\n",
    "site_name = None\n",
    "try:\n",
    "    if not SLICE_EXISTS:\n",
    "        num_nodes = len(resources_needed.keys())\n",
    "        site_dict_list = fablib.list_sites(output='list', quiet=True)\n",
    "        for site_dict in site_dict_list:\n",
    "            print(site_dict['name'] + ', shared_nics_available --> ' + str(site_dict['nic_basic_available']) + ', cpu available--> '+ str(site_dict['cores_available']) + ', ram available --> ' + str(site_dict['ram_available']))\n",
    "            if (site_dict['nic_basic_available'] > (1 * num_nodes) and site_dict['cores_available'] > (4 * num_nodes) and site_dict['ram_available'] > (80 * num_nodes)):\n",
    "                site_name = site_dict['name']\n",
    "                print(f'Site being used will be: {site_name}')\n",
    "                break\n",
    "        print(f\"A slice does not exist - Will create a new slice with the name {SLICENAME} at site: {site_name}\")\n",
    "    else:\n",
    "        print(f\"A slice does already exists\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f653d8-65cf-476a-b98f-b9bfa63f36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our experiment wored exceptionally well at UCSD site\n",
    "site_name = 'UCSD'\n",
    "print(f\"site_name is being overridden to {site_name} to avoid 500 errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964793bb-57c9-4e1e-91d9-90f90c8f4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    if SLICE_EXISTS:\n",
    "        print(\"You already have a slice named %s.\" % SLICENAME)\n",
    "        fabric_slice = fablib.get_slice(name=SLICENAME)\n",
    "        for node in fabric_slice.get_nodes():\n",
    "            print(f\"{node}\")\n",
    "    else:\n",
    "        print(f\"creating new nodes and assigning to the slice {SLICENAME}\")\n",
    "        node1 = fabric_slice.add_node(name = 'node1', site=site_name, cores=resources_needed['node1']['cpu'], ram=resources_needed['node1']['ram'], disk=60, image='default_ubuntu_20')\n",
    "        node2 = fabric_slice.add_node(name = 'node2', site=site_name, cores=resources_needed['node2']['cpu'], ram=resources_needed['node2']['ram'], disk=60, image='default_ubuntu_20')\n",
    "        ifacenode1 = node1.add_component(model=\"NIC_Basic\", name='if_node_1').get_interfaces()[0]\n",
    "        ifacenode2 = node2.add_component(model=\"NIC_Basic\", name='if_node_2').get_interfaces()[0]\n",
    "        net1 = fabric_slice.add_l2network(name='net_1', type='L2Bridge', interfaces=[ifacenode1, ifacenode2])\n",
    "        fabric_slice.submit()\n",
    "except Exception as err:\n",
    "    print(sys.exc_info())\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48709655-736f-4d75-a33e-c9df340ae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric_slice.list_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86485b96-f8e9-4864-b377-59fef7123d2b",
   "metadata": {},
   "source": [
    "### Our Topology\n",
    "\n",
    "Our FABRIC VM Topology is the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0cb5f-a134-4f35-a18a-420dba78d907",
   "metadata": {},
   "source": [
    "### The commands to setup the NDN-DPDK Dependencies and the Program (Manual Via SSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed0ebf-d71c-4ae9-96cb-dac582a87c68",
   "metadata": {},
   "source": [
    "This code should run inside the ssh session of the remote server that is initiated by the experimenter. I could have used the fabric primitives to run the commands remotely, but I was not able to get deterministic results everytime i.e. our command would time out in the middle of execution causing the VM to be left in an inconsistent state, which would require us to reprovision the VM. Therefore, we chose to list out the commands in the form of a bash script. You can create a `setup-ndn.sh` bash script in the home directory of your session after SSH-ing into the VM. Provide execute permissions to the script using `chmod 500 setup-ndn.sh` command and then run the script using `./setup-ndn.sh`.\n",
    "\n",
    "> Run the commands __without__ `sudo` or a `root` shell. It should run within a normal shell under the default ubuntu user provisioned by the fabric VMs; that's the only way `ndndpdk-depends.sh` will work.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "export SELF_DIR=$PWD\n",
    "sudo apt update\n",
    "wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-5.8-1.0.1.1/MLNX_OFED_SRC-debian-5.8-1.0.1.1.tgz\n",
    "tar zxvf MLNX_OFED_SRC-debian-5.8-1.0.1.1.tgz\n",
    "sudo MLNX_OFED_SRC-5.8-1.0.1.1/./install.pl\n",
    "git clone https://github.com/usnistgov/ndn-dpdk\n",
    "git clone https://github.com/DPDK/dpdk\n",
    "sudo apt install --no-install-recommends -y ca-certificates curl jq lsb-release sudo nodejs\n",
    "chmod a+x ndn-dpdk/docs/ndndpdk-depends.sh\n",
    "echo | ndn-dpdk/docs/./ndndpdk-depends.sh\n",
    "sudo npm install -g pnpm\n",
    "cd ndn-dpdk/core && pnpm install\n",
    "cd $SELF_DIR/ndn-dpdk && NDNDPDK_MK_RELEASE=1 make && sudo make install \n",
    "cd $SELF_DIR && sudo python3 $SELF_DIR/dpdk/usertools/dpdk-hugepages.py -p 1G --setup 64G\n",
    "sudo ndndpdk-ctrl systemd start\n",
    "ndndpdk-ctrl -v\n",
    "```\n",
    "#### Test for Successful Execution\n",
    "run `ndndpdk-ctrl -v` and it should return an output similar to below:\n",
    "```bash\n",
    "ndndpdk-ctrl version v0.0.0-20230411150822-eae4b29557ea\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7be5f-b778-4d03-9df3-ec5d911ea95b",
   "metadata": {},
   "source": [
    "### Setting up NDN-DPDK as a forwarder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ddfde-8649-4a71-94a4-4879b62700c2",
   "metadata": {},
   "source": [
    "We need to manually program ndn-dpdk to act as a forwarder AND then link the 2 NDN DPDK instances together. After the NDN-DPDK program has been configured to work in the forwarder mode, we need to create `eth-port` and an `eth-face`. The creation of the `eth-port` assigns the shared-nic present on the system to be controlled by the DPDK program using one of the `pci`, `A_NET` or the `XDP` drivers (the commands below chose the `PCI` driver since it is the fastest and also the one compatible with the NVIDIA Mellanox Connect-X6 cards used in FABRIC [Network Hardware](https://learn.fabric-testbed.net/knowledge-base/network-interfaces-in-fabric-vms/)). One `eth-port` instance can be associated with multiple `eth-iface` instances. The `eth-face` is a generalization of a network interface, which is used by NDN-DPDK programs to connect and forward application. more here: [https://github.com/usnistgov/ndn-dpdk/blob/main/docs/face.md](https://github.com/usnistgov/ndn-dpdk/blob/main/docs/face.md).\n",
    "Think of each `eth-face` as a dedicated connection to the other node. We have to run the following commands on every pair of NDN-DPDK instances that we want to connect to each other. The `echo {} | ndndpdk-ctrl activate-forwarder` command only has to be run once per VM. The rest of the commands have to be every time a new interface is added to the instance that is running the NDN-DPDK program. the `mtu` parameter activates jumbo layer-2 frames.\n",
    "```bash\n",
    "echo {} | ndndpdk-ctrl activate-forwarder\n",
    "sudo ndndpdk-ctrl create-eth-port --pci <INTERFACE INDEX> # replace <interface index> with the following format\n",
    "sudo ndndpdk-ctrl create-ether-face --local <LOCAL INTERFACE MAC> --remote <OTHER NODES INTERFACE MAC> # replace with mac address of local ConnectX6 interface  and mac address of remote ConnectX6 interface. \n",
    "```\n",
    "We will need to save the output of the IDs generated by each of these commands. We will refer to them as `$NODE1_IFACEID` and `NODE2_IFACEID` from now on. It would be helpful to save these into __persistent__ environment viariables on both the servers from now on.    \n",
    "\n",
    "> __Note:__ Now or in the future if the `ndndpdk-ctrl create-{ether-/udp-/-}face` commands fail please restart the ndn-dpdk service. Failed create-face commands leave the the NDN-DPDK instance in an inconsistent state, and restarting the system service resolves that. \n",
    ">> __finding the interface index:__ In order to find the `<INTERFACE INDEX>` for the `--pci` driver to work use the `lspci` command in the FABRIC VMs Shell. In our experience it is always at location `00:07.00` under the name `Mellanox Technologies MT28908 Family [ConnectX-6 Virtual Function]` as shown in the output below:\n",
    "```bash\n",
    "    ubuntu@df286671-d1dd-42bc-90fe-f78324a8a0e6-node1:~$ lspci\n",
    "    00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)\n",
    "    00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]\n",
    "    00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]\n",
    "    00:01.2 USB controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)\n",
    "    00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)\n",
    "    00:02.0 VGA compatible controller: Cirrus Logic GD 5446\n",
    "    00:03.0 Ethernet controller: Red Hat, Inc. Virtio network device\n",
    "    00:04.0 SCSI storage controller: Red Hat, Inc. Virtio block device\n",
    "    00:05.0 Unclassified device [00ff]: Red Hat, Inc. Virtio memory balloon\n",
    "    00:06.0 Unclassified device [00ff]: Red Hat, Inc. Virtio RNG\n",
    "    00:07.0 Ethernet controller: Mellanox Technologies MT28908 Family [ConnectX-6 Virtual Function]\n",
    "```\n",
    ">>__finding the mac address:__ the mac address can be found using the `ifconfig` command and looking at the mac address of the interface associated with the ConnectX6 NIC. it has usually been named `ens7np0` for out experiments. it is also the interface that should show the status `DOWN`.\n",
    "\n",
    "#### Testing for successful execution\n",
    "We can test if the NDN-DPDK instances are running by running the following commands. on `node2` we will run the the ping server provided by NDN-DPDK package itself, while the `node1` acts as the client. the NDN-DPDK program includes the `ndndpdk-godemo` which simulates both the server and client. \n",
    "First we will feed the the FIB entry in `node1` by running the following command:\n",
    "\n",
    "```bash\n",
    "ndndpdk-ctrl insert-fib --name /example/Ping --nh $NODE1_IFACEID\n",
    "```\n",
    "\n",
    "The command creates a Forwarding Information Base (FIB) Entry in the NDN-DPDK forwarder by telling it to forward the interests that it receives with the name `/testNode2/Ping` to the next-hop (`--nh`, get it?) address referenced by the `eth-face` ID we created in the earlier step. Then on `node2` start the pingserver process that's included in the ndndpdk-godemo binary - this should start a producer which produces dummy packets for the name `/example/Ping`.\n",
    "```bash\n",
    "ndndpdk-godemo pingserver --name /example/Ping\n",
    "```\n",
    "Next we start the `pingclient` process on `node1` to send to the namespace `/example/Ping`.\n",
    "\n",
    "```bash\n",
    "ndndpdk-godemo pingserver --name /example/Ping\n",
    "```\n",
    "\n",
    "The output on `node1` is shown below: \n",
    "```bash\n",
    "ubuntu@df286671-d1dd-42bc-90fe-f78324a8a0e6-node1:~$ ndndpdk-godemo pingclient --name /example/Ping\n",
    "2023/05/05 00:52:23 uplink opened, state is down\n",
    "2023/05/05 00:52:23 uplink state changes to up\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A38326   7853us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A38327   8028us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A38328   8021us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A38329   6944us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A3832A   6688us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A3832B    830us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A3832C   1090us\n",
    "2023/05/05 00:52:23 100.00% D F2AA3C9E08A3832D   1196us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A3832E   1513us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A3832F    804us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38330   1025us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38331   1366us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38332   1512us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38333   1784us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38334   2170us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38335   1377us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38336    732us\n",
    "2023/05/05 00:52:24 100.00% D F2AA3C9E08A38337    953us\n",
    "2023/05/05 00:52:25 100.00% D F2AA3C9E08A38338   1224us\n",
    "2023/05/05 00:52:25 100.00% D F2AA3C9E08A38339   1540us\n",
    "2023/05/05 00:52:25 100.00% D F2AA3C9E08A3833A    672us\n",
    "^C2023/05/05 00:52:25 uplink state changes to down\n",
    "2023/05/05 00:52:25 uplink closed, error is <nil>\n",
    "```\n",
    "\n",
    "The output on `node2` is shown below: \n",
    "\n",
    "```bash\n",
    "ubuntu@a256c903-4c24-4ac7-af1c-53ed6dcaefa3-node2:~$ ndndpdk-godemo pingserver --name /example/Ping\n",
    "2023/05/05 00:51:28 uplink opened, state is down\n",
    "2023/05/05 00:51:28 uplink state changes to up\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A38326[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A38327[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A38328[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A38329[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A3832A[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A3832B[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A3832C[F]\n",
    "2023/05/05 00:52:23 /8=example/8=Ping/8=F2AA3C9E08A3832D[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A3832E[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A3832F[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38330[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38331[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38332[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38333[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38334[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38335[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38336[F]\n",
    "2023/05/05 00:52:24 /8=example/8=Ping/8=F2AA3C9E08A38337[F]\n",
    "2023/05/05 00:52:25 /8=example/8=Ping/8=F2AA3C9E08A38338[F]\n",
    "2023/05/05 00:52:25 /8=example/8=Ping/8=F2AA3C9E08A38339[F]\n",
    "2023/05/05 00:52:25 /8=example/8=Ping/8=F2AA3C9E08A3833A[F]\n",
    "\n",
    "^C2023/05/05 01:01:33 uplink closed, error is <nil>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8838242-d2c9-4293-a9db-97e889602a82",
   "metadata": {},
   "source": [
    "### Installing an alternate version of python on the FABRIC VM \\[For Using Python NDN\\]\n",
    "We have used ubuntu 20.04 lts version as our OS - but we need python>=3.9 to compile the newest version of the `python-ndn` library from the source. Ubuntu 20.04 (focal) only ships with pythonv3.8 - and we need python 3.11 so we perform the following steps. \n",
    "```bash\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "# then press enter to agree to add the repository\n",
    "sudo apt install python3.11\n",
    "# now you can call python3.11 by explicitly using python3.11 command. don't use python3 or python which will call the legacy version of pyhton.\n",
    "curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "python3.11 get-pip.py\n",
    "# this will install pip version compatible with python3.11 in your /home/${USER}/.local/bin directory we need to add this to the path\n",
    "# CAUTION: This step has to be performed EVERY TIME you open the terminal/New session. If you don't want to do that add it to your PATH\n",
    "# using export PATH=\"$PATH:/home/ubuntu/.local/bin\" command in the end of your .bashrc in the home directory.\n",
    "export PATH=\"$PATH:/home/ubuntu/.local/bin\"\n",
    "# Call pip using pip3.11\n",
    "pip3.11 --version\n",
    "# call python using python3.11\n",
    "python3.11 --version\n",
    "```\n",
    "\n",
    "Now we install the python-ndn package using pip on both the VMs. \n",
    "```bash\n",
    "pip3.11 install \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b6211-91e2-481e-aae0-c3c09ec0c848",
   "metadata": {},
   "source": [
    "### Install Docker Engine and Fetching the NFD Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f464b-4757-49be-b567-a11899ebb29f",
   "metadata": {
    "tags": []
   },
   "source": [
    "The authors of NDN DPDK recommend we run the NFD as a We can install the Docker Engine application on the servers using the following commands:\n",
    "```bash\n",
    "sudo apt-get update && \\\n",
    "sudo apt-get install  ca-certificates curl gnupg && \\\n",
    "sudo install -m 0755 -d /etc/apt/keyrings && \\\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg && \\\n",
    "sudo chmod a+r /etc/apt/keyrings/docker.gpg && \\\n",
    "echo \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n",
    "\"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null && \\\n",
    "sudo apt-get update && \\\n",
    "sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin && \\\n",
    "sudo usermod -aG docker ${USER}\n",
    "```\n",
    "#### Test for successful execution\n",
    "Then we can download the `hello-world` docker image to check if it's working fine!\n",
    "```bash\n",
    "sudo docker run hello-world\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cc705-bf2a-4943-b2a3-ba3a67de8345",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setting up the NFD Container\n",
    "Now we need to download the image needed to make NFD work. The authors of NDN-DPDK suggest using a dockerized image of NFD so it can function in isolation from the other packages, which prevents a lot of the common errors that occur because of mismatched or interfering dependencies.\n",
    "\n",
    "Go back to the directory where the `ndn-dpdk` folder has been cloned - It should most probably be the home directory.\n",
    "```bash\n",
    "sudo docker build --pull -t nfd ndn-dpdk/docs/interop/nfd/.\n",
    "```\n",
    "You should get an output similar to what is shown below - The key part in this output is that it is built using the `debian:bullseye` base image; which means it has a stripped down version of debian and accompanying programs, which needed to initialize a shell that we can use to develop our stub program. \n",
    "```bash\n",
    "[+] Building 61.1s (9/9) FINISHED                                                                                                           \n",
    " => [internal] load build definition from Dockerfile                                                                                        \n",
    " => => transferring dockerfile: 611B                                                                                                        \n",
    " => [internal] load .dockerignore                                                                                                           \n",
    " => => transferring context: 2B                                                                                                             \n",
    " => [internal] load metadata for docker.io/library/debian:bullseye                                                                           \n",
    " => [1/4] FROM docker.io/library/debian:bullseye@sha256:63d62ae233b588d6b426b7b072d79d1306bfd02a72bff1fc045b8511cc89ee09                   \n",
    " => => resolve docker.io/library/debian:bullseye@sha256:63d62ae233b588d6b426b7b072d79d1306bfd02a72bff1fc045b8511cc89ee09                   \n",
    " => => sha256:63d62ae233b588d6b426b7b072d79d1306bfd02a72bff1fc045b8511cc89ee09 1.85kB / 1.85kB                                             \n",
    " => => sha256:32888a3c745e38e72a5f49161afc7bb52a263b8f5ea1b3b4a6af537678f29491 529B / 529B                                                 \n",
    " => => sha256:34b4fa67dc04381e908b662ed69b3dbe8015fa723a746c66cc870a5330520981 1.46kB / 1.46kB                                             \n",
    " => => sha256:918547b9432687b1e1d238e82dc1e0ea0b736aafbf3c402eea98c6db81a9cb65 55.05MB / 55.05MB                                           \n",
    " => => extracting sha256:918547b9432687b1e1d238e82dc1e0ea0b736aafbf3c402eea98c6db81a9cb65                                                  \n",
    " => [internal] load build context                                                                                                          \n",
    " => => transferring context: 1.19kB                                                                                                        \n",
    " => [2/4] RUN apt-get -y -qq update  && apt-get -y -qq install --no-install-recommends ca-certificates  && echo \"deb [arch=amd64 tr...\" 35.2s\n",
    " => [3/4] COPY start.sh /                                                                                                                  \n",
    " => [4/4] RUN chmod +x /start.sh  && setcap -r /usr/bin/nfd || true  && echo 'transport=unix:///run/ndn/nfd.sock' > /etc/ndn/client.conf   \n",
    " => exporting to image                                                                                                                     \n",
    " => => exporting layers                                                                                                                    \n",
    " => => writing image sha256:8d7310a705391d6aa1127b0a9314d0d85e91a8d3851de22a401f5a8c73f95216                                               \n",
    " => => naming to docker.io/library/nfd                                                                                                      \n",
    "```\n",
    "This should build and download the latest nightly container image of the NFD program. We will now initialize a container using the following command. We include the `--init` parameter because we want the container process to run under a init process. An init process has a PID 1 in the container. Specifying an init process ensures the usual responsibilities of an init system, such as reaping zombie processes, are performed inside the created container. \n",
    "```bash\n",
    "docker volume create run-ndn\n",
    "docker run -d --name nfd --network none --init --mount type=volume,source=run-ndn,target=/run/ndn -e 'NFD_CS_CAP=1024' nfd\n",
    "```\n",
    "> __Note:__ The Environment inside the docker container is __ephemeral__; which means all the data in it will be lost if the container stops or restarts. So make sure the changes made inside the container are stored somewhere safe, or mount volumes. \n",
    "#### Test for successful execution\n",
    "Now we check the status of our container using the `docker container ls` command -  you should see the nfd container running on the host.\n",
    "```bash\n",
    "ubuntu@8c853b4b-ad84-4676-895d-e636f5665705-node1:~$ docker container ls \n",
    "CONTAINER ID   IMAGE     COMMAND       CREATED       STATUS       PORTS     NAMES\n",
    "1bb1d3e24bc2   nfd       \"/start.sh\"   2 hours ago   Up 2 hours             nfd\n",
    "```\n",
    "If we want to open a shell we use the `docker exec -it nfd /bin/bash` command. This command runs a bash shell inside your container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ab04c-a7b7-4071-b1d0-cbe118d06dbb",
   "metadata": {},
   "source": [
    "### Exposing the NFD Socket in the container for NDN-DPDK and `python-ndn` to access\n",
    "\n",
    "We will now mount the socket that is running in the `/run/ndn` location of the NFD container and expose it on the host which is running the container, the NDN-DPDK program and will run the python program as well. \n",
    "\n",
    "We will first take a look at the `run-ndn` doocker volume which we created for the container on each host - we should run the `docker volume inspect run-ndn` command which should give us the __actual directory on the disk of the container host__, which is mounted in the `/run/ndn` location inside container. That directory has the `nfd.sock` object - which is the nfd socket. We need to mount that directory in the `/run/ndn` of our container/NDN-DPDK host. \n",
    "\n",
    "```bash\n",
    "# output of docker volume inspect - we can the directory /var/lib/docker/volumes/run-ndn/_data is the directory mounted in /run/ndn\n",
    "\n",
    "    {\n",
    "        \"CreatedAt\": \"2023-05-04T16:54:42Z\",\n",
    "        \"Driver\": \"local\",\n",
    "        \"Labels\": null,\n",
    "        \"Mountpoint\": \"/var/lib/docker/volumes/run-ndn/_data\",\n",
    "        \"Name\": \"run-ndn\",\n",
    "        \"Options\": null,\n",
    "        \"Scope\": \"local\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Now we mount that directory in the `/run/ndn` directory of the our FABRIC VMs. \n",
    "\n",
    "```bash\n",
    "sudo mkdir -p /run/ndn\n",
    "sudo mount --bind $(docker volume inspect -f '{{.Mountpoint}}' run-ndn) /run/ndn\n",
    "```\n",
    "\n",
    "> Note: This mount is emphemeral - will not persist across system reboots. If you want a permanent mount look into adding the entry into `/etc/fstab` along with making sure the docker container starts before the mount happens otherwise the `/etc/fstab` mount will fail.\n",
    "\n",
    "#### Test for successful execution\n",
    "Once the docker volume has been mounted on the `/run/ndn`, you should see the `nfd.sock` file once you run `ls -lh /run/ndn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35307d62-d68e-4f3e-8b14-1b097cfbc8d0",
   "metadata": {},
   "source": [
    "### Setting up the Server Nodes\n",
    "We will be using the `node1` as a client and `node2` as the server. our server program will serve the `/huffpost/article` namespace. the topology is shown below:\n",
    "![](./images/topology.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b76ad3-5353-4c25-a9c8-70b3bde344c5",
   "metadata": {},
   "source": [
    "In order to achieve this we will have to make sure the requests originating form the consumer process on node 2 travel through the NFD to the NDN-DPDK instance on `node2` to `node1` upstream and then to the NFD instance on `node1`,  which then passes it to the producer python application.\n",
    "#### Registering the NFD Process with NDN-DPDK.\n",
    "WE need to register the NFD instances with NDN-DPDK forwarders. WE can think of the NFD instances as customer routers with reduced capacity and speed, while the NDN-DPDK forwarders make up the core of the network. \n",
    "```bash\n",
    "jq -n '{scheme: \"unix\", remote:\"/run/ndn/nfd.sock\"}' | ndndpdk-ctrl create-face\n",
    "```\n",
    "save the output ID of this command as `NODE1_NFD_FACEID` and `NODE2_NFS_FACEID` when running on both nodes. Once this is done we can check if the connection was successful by setting up a server process inside the NFD container on `node1` for a sample ping, and send a ping from the NFD container in node 2 to the NFD container in node 1. lets set up a series of next hop paths from NFD_Node2 --> NDN-DPDK_Node2 --> NDN-DPDK_Node1 --> NFD_Node1. We will use the `ndnping` and `ndnpingserver` commands (provided with the NFD containers) inside the Node2 NFD Container and Node 1 NFD Container respectively. Out ping name for the FIB will be `/example/Ping2` \n",
    "\n",
    "> __Hint:__ if you forgot to set the environment variables for the `eth-faces` you can always retrieve them using the `ndndpdk-ctrl list-face` command\n",
    "\n",
    "```bash\n",
    "# register the route on the NDN-DPDK2 to NDN-DPDK Node 1 route.\n",
    "ndndpdk-ctrl insert-fib --name /example/Ping2 --id $NODE2\n",
    "# register the route on the NDN-DPDK node1 to NFD Node 1 route.\n",
    "ndndpdk-ctrl insert-fib --name /example/Ping2 --id $NODE1_NFD_IFACEID\n",
    "```\n",
    "\n",
    "Now we start the the `ndnpingserver` in the NFD container on `node1`.\n",
    "```bash\n",
    "# activate the NFD container on node1 - might need sudo if you are not part of docker group\n",
    "docker exec -it nfd /bin/bash\n",
    "# run the ping server with the namespace /example/Ping2\n",
    "ndnpingserver --size 512 /example/Ping2\n",
    "```\n",
    "Now we start the `ndnping` application inside the NFD container on `node2`; However, there is an extra step. How does the NFD container on `node2` know to forward the packet from inside the container to the NDN-DPDK application? For this we first register the `eth-face` between NDN-DPDK on node2 to route NDN packets from the NDN-DPDK on `node2` to the NFD container; the packets will be addressed to the `/localhost/nfd` namespace; This is because the prefix registerer in the NFD container is able to accept packets over the network - once we do that we use the `ndndpdk-godemo` application to register the prefix using the `nfdreg` subroutine:\n",
    "```bash\n",
    "# registering the prefix in the fib of node2 to connect to the NFD's prefix regieterer.\n",
    "ndndpdk-ctrl insert-fib --name /localhost/nfd --nh $NODE2_NFD_IFACEID\n",
    "# register the prefix to /example/Ping2 as going through node2 NDN-DPDK \n",
    "# - basically just setting the next hop address for /example/Ping2 to the interface through which this packet is received.\n",
    "ndndpdk-godemo nfdreg --command /localhost/nfd --origin 0 --register /example/Ping2\n",
    "```\n",
    "A successful registration should show the following output.\n",
    "```bash\n",
    "ubuntu@a256c903-4c24-4ac7-af1c-53ed6dcaefa3-node2:/run/ndn$ ndndpdk-godemo nfdreg --command /localhost/nfd --origin 0 --register /example/Ping2\n",
    "2023/05/05 03:02:15 uplink opened, state is down\n",
    "2023/05/05 03:02:15 uplink state changes to up\n",
    "2023/05/05 03:02:17 200 nfdmgmt.RibRegisterCommand {\"name\":\"/8=example/8=Ping2\",\"origin\":0,\"cost\":0,\"noInherit\":true,\"capture\":true}\n",
    "2023/05/05 03:02:18 uplink closed, error is <nil>\n",
    "```\n",
    "\n",
    "NOw we activate the `ndnping` command:\n",
    "```bash\n",
    "# activate the NFD container on node2 - might need sudo if you are not part of docker group\n",
    "docker exec -it nfd /bin/bash\n",
    "# run the ping server with the namespace /example/Ping2\n",
    "ndnping --size 512 /example/Ping2\n",
    "```\n",
    "\n",
    "A successful output is as follows:\n",
    "```bash\n",
    "# on node1 NFD - The ping-ee\n",
    "ubuntu@df286671-d1dd-42bc-90fe-f78324a8a0e6-node1:/run/ndn$ docker exec -it nfd /bin/bash\n",
    "root@d4869c55680b:/# ndnpingserver --size 512 /example/Ping2\n",
    "PING SERVER /example/Ping2\n",
    "interest received: seq=6812403586528848708\n",
    "interest received: seq=6812403586528848709\n",
    "interest received: seq=6812403586528848710\n",
    "interest received: seq=6812403586528848711\n",
    "interest received: seq=6812403586528848712\n",
    "interest received: seq=6812403586528848713\n",
    "interest received: seq=6812403586528848714\n",
    "...\n",
    "```\n",
    "\n",
    "```bash\n",
    "# on node2 NFD - The ping-er\n",
    "ubuntu@a256c903-4c24-4ac7-af1c-53ed6dcaefa3-node2:/run/ndn$ docker exec -it nfd /bin/bash\n",
    "root@63393ce4a3da:/# ndnping -i 10 /example/Ping2\n",
    "PING /example/Ping2\n",
    "content from /example/Ping2: seq=6812403586528848708 time=2.75585 ms\n",
    "content from /example/Ping2: seq=6812403586528848709 time=0.413672 ms\n",
    "content from /example/Ping2: seq=6812403586528848710 time=0.389707 ms\n",
    "content from /example/Ping2: seq=6812403586528848711 time=0.403603 ms\n",
    "content from /example/Ping2: seq=6812403586528848712 time=0.404986 ms\n",
    "content from /example/Ping2: seq=6812403586528848713 time=0.365221 ms\n",
    "content from /example/Ping2: seq=6812403586528848714 time=0.408753 ms\n",
    "content from /example/Ping2: seq=6812403586528848715 time=0.405627 ms\n",
    "...\n",
    "```\n",
    "\n",
    "This demonstrates end-to-end connectivity between NFD instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56bbb6-fa4f-4cac-b204-de4833c447d7",
   "metadata": {},
   "source": [
    "### Simple Producer and Consumer bind to the NFD node.\n",
    "\n",
    "#### Difficulties encountered while creating the program\n",
    "- Finding the **correct** sample programs was a bit difficult because the documentation for python-ndn is out of date (latest release as of 05/05/23 is version:v0.3-1). The actual sample code is tucked away in their [repository](https://github.com/named-data/python-ndn/tree/master/examples/appv2/basic_packets). This is the sample code on the README docs website [https://python-ndn.readthedocs.io/en/latest/src/readme.html](https://python-ndn.readthedocs.io/en/latest/src/readme.html).\n",
    "- Moreover, the last release hosted on `pip` does not include the `appv2` class, which actually supports the PIT token - We would have to install from source using the `pip3 install -U -U git+https://github.com/named-data/python-ndn.git`, and use the `appv2` version of the `NDNApp` to make it work with our setup.\n",
    "- Another issue we faced once we got the code working was that it could not find the keystore - `pib.db` sqlite3 file, which has the keystore for verifying the authenticity of the packets. We got around that error system by looking for the [`pib.db`](https://github.com/named-data/python-ndn/blob/master/examples/lvs/pib.db) file in their code, and found one that seems to work.\n",
    "\n",
    "#### Modifying the NFD Docker Image v1.0\n",
    "> Requires Docker Engine to be installed on your FABRIC VM, and a already built `nfd` docker image from the earlier section.\n",
    "\n",
    "> __Disclaimer:__ Bundling these tools into a production container image is frowned upon; It makes them bulky, and outdated packages (because of the immutable nature of container images) often increase the attack surface of a container. However, our objective is a experiment so it is for fine now.\n",
    "\n",
    "We will modify the stock docker image that is given to us from the NDN-DPDK NFD interop section and we will modify it modify the docker image to include the `python` binaries, the `pip` python package manager, `git` and text editor for debugging our code.\n",
    "\n",
    "```Dockerfile\n",
    "FROM nfd:latest\n",
    "\n",
    "RUN apt-get update -y\n",
    "RUN apt-get install -y python3 python3-dev python3-pip less vim nano\n",
    "RUN apt-get install -y git wget\n",
    "RUN pip3 install -U git+https://github.com/named-data/python-ndn.git\n",
    "```\n",
    "\n",
    "Then build the container using the `docker build -t nfdv2:test` command and run the container using the same set. Go into the container using `docker exec -it nfdv2 /bin/bash` to start testing and developing our script.\n",
    "#### The Sample Producer Code\n",
    "\n",
    "Our sample producer code is shown here. it is a very straightforward implementation, and might also look familiar to someone who has used [Flask](https://flask.palletsprojects.com/en/2.3.x/) or [Django](https://www.djangoproject.com/). We register a handler for the prefix `/example/TestApp` prefix, and everytime the NFD instance receives an interest packet destined for that prefix, it forwards it to the handler. The Handler prints the Interest onto the screen, and then generates a `Hello World` binary encoded response for the requester. \n",
    "\n",
    "```python\n",
    "# -----------------------------------------------------------------------------\n",
    "# Copyright (C) 2019-2022 The python-ndn authors\n",
    "#\n",
    "# This file is part of python-ndn.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# -----------------------------------------------------------------------------\n",
    "import typing\n",
    "import logging\n",
    "from ndn import appv2\n",
    "from ndn import encoding as enc\n",
    "\n",
    "\n",
    "logging.basicConfig(format='[{asctime}]{levelname}:{message}',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    style='{')\n",
    "\n",
    "\n",
    "app = appv2.NDNApp()\n",
    "keychain = app.default_keychain()\n",
    "\n",
    "\n",
    "@app.route('/example/testApp')\n",
    "def on_interest(name: enc.FormalName, _app_param: typing.Optional[enc.BinaryStr],\n",
    "                reply: appv2.ReplyFunc, context: appv2.PktContext):\n",
    "    print(f'>> I: {enc.Name.to_str(name)}, {context[\"int_param\"]}')\n",
    "    content = \"Hello, world!\".encode()\n",
    "    reply(app.make_data(name, content=content, signer=keychain.get_signer({}),\n",
    "                        freshness_period=10000))\n",
    "    print(f'<< D: {enc.Name.to_str(name)}')\n",
    "    print(enc.MetaInfo(freshness_period=10000))\n",
    "    print(f'Content: (size: {len(content)})')\n",
    "    print('')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_forever()\n",
    "```\n",
    "\n",
    "#### The sample consumer code.\n",
    "\n",
    "The Sample consumer code is shown below - It is pretty simple in functionality; sends out a a interest packet to the prefix `/example/testApp` but appended with `randomData/<timestamp>` suffix.\n",
    "\n",
    "```bash\n",
    "# -----------------------------------------------------------------------------\n",
    "# Copyright (C) 2019-2022 The python-ndn authors\n",
    "#\n",
    "# This file is part of python-ndn.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# -----------------------------------------------------------------------------\n",
    "import logging\n",
    "from ndn import utils, appv2, types\n",
    "from ndn import encoding as enc\n",
    "\n",
    "\n",
    "logging.basicConfig(format='[{asctime}]{levelname}:{message}',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    style='{')\n",
    "\n",
    "\n",
    "app = appv2.NDNApp()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        timestamp = utils.timestamp()\n",
    "        name = enc.Name.from_str('/example/testApp/randomData') + [enc.Component.from_timestamp(timestamp)]\n",
    "        print(f'Sending Interest {enc.Name.to_str(name)}, {enc.InterestParam(must_be_fresh=True, lifetime=6000)}')\n",
    "        # TODO: Write a better validator\n",
    "        data_name, content, pkt_context = await app.express(\n",
    "            name, validator=appv2.pass_all,\n",
    "            must_be_fresh=True, can_be_prefix=False, lifetime=6000, no_signature=True)\n",
    "\n",
    "        print(f'Received Data Name: {enc.Name.to_str(data_name)}')\n",
    "        print(pkt_context['meta_info'])\n",
    "        print(bytes(content) if content else None)\n",
    "    except types.InterestNack as e:\n",
    "        print(f'Nacked with reason={e.reason}')\n",
    "    except types.InterestTimeout:\n",
    "        print(f'Timeout')\n",
    "    except types.InterestCanceled:\n",
    "        print(f'Canceled')\n",
    "    except types.ValidationFailure:\n",
    "        print(f'Data failed to validate')\n",
    "    finally:\n",
    "        app.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_forever(after_start=main())\n",
    "```\n",
    "\n",
    "#### But it still doesn't work - WHY?!\n",
    "Running `python test_producer.py` does successfully start up the producer, and we even see the producer propogate the routes to the NFD process as shown in the log entries (you can view them using `docker logs nfdv2 --follow`) below:\n",
    "```bash\n",
    "1683301433.582420  INFO: [nfd.FaceTable] Added face id=258 remote=fd://18 local=unix:///run/ndn/nfd.sock\n",
    "1683301433.582996  INFO: [nfd.RibManager] Adding route /example/testApp nexthop=258 origin=app cost=0\n",
    "1683302260.817791  INFO: [nfd.Transport] [id=258,local=unix:///run/ndn/nfd.sock,remote=fd://18] setState UP -> CLOSING\n",
    "```\n",
    "but when we run the `python test_consumer.py` the program crashes with the following output:\n",
    "```bash\n",
    "root@58d91e6313bb:~# python3 test_consumer.py \n",
    "Sending Interest /example/testApp/randomData/t=1683316076190, InterestParam(can_be_prefix=False, must_be_fresh=True, nonce=None, lifetime=6000, hop_limit=None, forwarding_hint=[])\n",
    "Timeout\n",
    "[2023-05-05 19:48:02]INFO:Manually shutdown\n",
    "```\n",
    "and the producer was not doing any better - the output on the producer side was as follows:\n",
    "```bash\n",
    "root@58d91e6313bb:~# python3 test_producer.py \n",
    ">> I: /example/testApp/randomData/t=1683318604389, InterestParam(can_be_prefix=False, must_be_fresh=True, nonce=3374275792, lifetime=6000, hop_limit=None, forwarding_hint=[])\n",
    "[2023-05-05 20:30:04]ERROR:Task exception was never retrieved\n",
    "future: <Task finished name='Task-6' coro=<NDNApp._on_interest.<locals>.submit_interest() done, defined at /usr/local/lib/python3.9/dist-packages/ndn/appv2.py:360> exception=KeyError(b'\\x07(\\x08\\x08lvs-test\\x08\\x06author\\x08\\x05xinyu\\x08\\x03KEY\\x08\\x08\\x18\\xf9\\xa7CP\\xf6\\xbd\\x1b')>\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.9/dist-packages/ndn/appv2.py\", line 372, in submit_interest\n",
    "    node.callback(name, app_param, reply, context)\n",
    "  File \"/root/test_producer.py\", line 43, in on_interest\n",
    "    reply(app.make_data(name, content=content, signer=keychain.get_signer({}),\n",
    "  File \"/usr/local/lib/python3.9/dist-packages/ndn/security/keychain/keychain_sqlite3.py\", line 639, in get_signer\n",
    "    signer = self.tpm.get_signer(key_name, key_locator_name)\n",
    "  File \"/usr/local/lib/python3.9/dist-packages/ndn/security/tpm/tpm_file.py\", line 49, in get_signer\n",
    "    raise KeyError(key_name)\n",
    "KeyError: b'\\x07(\\x08\\x08lvs-test\\x08\\x06author\\x08\\x05xinyu\\x08\\x03KEY\\x08\\x08\\x18\\xf9\\xa7CP\\xf6\\xbd\\x1b'\n",
    "```\n",
    "After 3 hours of painful troubleshooting we discovered that our issue was with the `pib.db` file that we provided. Our workaround did not work after all. The entity that signed the data packet for the consumer - The producer could not find a key within the `pib.db` file that we gave it. Setting the `signer=None`(inspired by https://github.com/named-data/python-ndn/issues/25) alleviated the issue but the NFD forwarder dropped responses that were unsigned (as shown here).\n",
    "```bash\n",
    "1683314514.410371  INFO: [nfd.FaceTable] Added face id=272 remote=fd://19 local=unix:///run/ndn/nfd.sock\n",
    "1683314514.418431  WARN: [nfd.GenericLinkService] [id=271,local=unix:///run/ndn/nfd.sock,remote=fd://18] packet parse error (SignatureInfo element is missing): DROP\n",
    "1683314520.414624  INFO: [nfd.Transport] [id=272,local=unix:///run/ndn/nfd.sock,remote=fd://19] setState UP -> CLOSING\n",
    "1683314520.414663  INFO: [nfd.Transport] [id=272,local=unix:///run/ndn/nfd.sock,remote=fd://19] setState CLOSING -> CLOSED\n",
    "```\n",
    "However, we came across another issue (completely  by accident - because I was reading through the issues out of frustration and desperation) where the user was facing a similar error - https://github.com/named-data/python-ndn/issues/28. They direct the user to another page which is not really helpful as it is the definition of another standard in NDN Spec; However, they mention that you can try with a SHA256 Digest of the message if you don't want to sign with a key - but the suggestion is not accomapnied by a link to the doc. So we perform a plaintext search on the docs hosted on the readme site and come across the [`KeychainDigest`](https://python-ndn.readthedocs.io/en/latest/src/security/security.html#module-ndn.security.keychain.keychain_digest) class, which can help us bypass the NFDs SHA256 signature requirement.\n",
    "\n",
    "#### Sample Producer and Consumer Code v.2.0\n",
    "\n",
    "test_producer.py\n",
    "```bash\n",
    "import typing\n",
    "import logging\n",
    "from ndn import appv2\n",
    "from ndn import encoding as enc\n",
    "from ndn.security.keychain.keychain_digest import KeychainDigest\n",
    "\n",
    "\n",
    "logging.basicConfig(format='[{asctime}]{levelname}:{message}',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    style='{')\n",
    "\n",
    "\n",
    "app = appv2.NDNApp()\n",
    "keychain = KeychainDigest()\n",
    "signer = keychain.get_signer({})\n",
    "\n",
    "\n",
    "@app.route('/example/testApp')\n",
    "def on_interest(name: enc.FormalName, _app_param: typing.Optional[enc.BinaryStr],\n",
    "                reply: appv2.ReplyFunc, context: appv2.PktContext):\n",
    "    print(f'>> I: {enc.Name.to_str(name)}, {context[\"int_param\"]}')\n",
    "    content = \"Hello, world!\".encode()\n",
    "    reply(app.make_data(name, content=content, signer=signer,\n",
    "                        freshness_period=10000))\n",
    "    print(f'<< D: {enc.Name.to_str(name)}')\n",
    "    print(enc.MetaInfo(freshness_period=10000))\n",
    "    print(f'Content: (size: {len(content)})')\n",
    "    print('')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_forever()\n",
    "```\n",
    "test_consumer.py\n",
    "```bash\n",
    "async def main():\n",
    "    try:\n",
    "        timestamp = utils.timestamp()\n",
    "        name = enc.Name.from_str('/example/testApp/randomData') + [enc.Component.from_timestamp(timestamp)]\n",
    "        print(f'Sending Interest {enc.Name.to_str(name)}, {enc.InterestParam(must_be_fresh=True, lifetime=6000)}')\n",
    "        # TODO: Write a better validator\n",
    "        data_name, content, pkt_context = await app.express(\n",
    "            name, validator=appv2.pass_all,\n",
    "            must_be_fresh=True, can_be_prefix=False, lifetime=6000, no_signature=True)\n",
    "\n",
    "        print(f'Received Data Name: {enc.Name.to_str(data_name)}')\n",
    "        print(pkt_context['meta_info'])\n",
    "        print(bytes(content) if content else None)\n",
    "    except types.InterestNack as e:\n",
    "        print(f'Nacked with reason={e.reason}')\n",
    "    except types.InterestTimeout:\n",
    "        print(f'Timeout')\n",
    "    except types.InterestCanceled:\n",
    "        print(f'Canceled')\n",
    "    except types.ValidationFailure:\n",
    "        print(f'Data failed to validate')\n",
    "    finally:\n",
    "        app.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_forever(after_start=main())\n",
    "```\n",
    "\n",
    "#### Test for successful execution\n",
    "and on running the file we get the following output on producer side of the container:\n",
    "\n",
    "![](./images/sample_producer.png)\n",
    "\n",
    "and the following output on the consumer side\n",
    "\n",
    "![](./images/sample_consumer.png)\n",
    "\n",
    "and we see the NFD forwarded the interest successfully, and the consumer receives the two beautiful words that every computer engineer/scientist yearns to read! Hello World.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76d3ff-44a1-41a0-99b4-2f8a2fee1671",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Our Program HUFFPOST ARCHIVES: Across the Network.\n",
    "We created a route from nfd on node2 to the nfd on node1 with the prefix `/huffpost/archives` for our toy archive program. the commands for routes have not been shown for brevity.\n",
    "Our producer program is shown below. it uses the news dataset form kaggle, and some of the important features of this producer program is that it will respond to interests with the prefix pattern `/huffpost/archives/YYYY/MM/DD` with the articles that were published on that date. It uses the `sqlite3` library to achieve this task. it first reads the 200K articles intosqlite table which is indexed by the date field, and then starts listening for the `/huffpost/archives` prefix. \n",
    "\n",
    "```bash\n",
    "# huffpost_producer.py\n",
    "import logging\n",
    "import typing\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import re\n",
    "from ndn import appv2\n",
    "from ndn import encoding as enc\n",
    "from ndn.security.keychain.keychain_digest import KeychainDigest\n",
    "\n",
    "logging.basicConfig(format='[{asctime}]{levelname}:{message}', datefmt='%Y-%m-%d %H:%M:%S', level=logging.INFO,\n",
    "                    style='{')\n",
    "\n",
    "app = appv2.NDNApp()\n",
    "keychain = KeychainDigest()\n",
    "signer = keychain.get_signer({})\n",
    "\n",
    "conn: sqlite3.Connection\n",
    "cur: sqlite3.Cursor\n",
    "\n",
    "date_pattern = \"^huffpost/archives/[0-9]{4}\\\\/[0-9]{1,2}\\\\/[0-9]{1,2}$\"\n",
    "\n",
    "\n",
    "@app.route('/huffpost/archives')\n",
    "def on_interest(name: enc.FormalName, _app_param: typing.Optional[enc.BinaryStr],\n",
    "                reply: appv2.ReplyFunc, context: appv2.PktContext):\n",
    "    user_interest = enc.Name.to_str(name)\n",
    "    content = match_interest_to_articles(user_interest)\n",
    "    print(f'>> I: {enc.Name.to_str(name)}, {context[\"int_param\"]}')\n",
    "    if content:\n",
    "        content = content.encode()\n",
    "    else:\n",
    "        content = '404 - NOT FOUND! Please send a Valid Date'.encode()\n",
    "    reply(app.make_data(name, content=content, signer=signer,\n",
    "                        freshness_period=100000))\n",
    "    print(f'<< D: {enc.Name.to_str(name)}')\n",
    "    print(enc.MetaInfo(freshness_period=100000))\n",
    "    print(f'Content: (size: {len(content)})')\n",
    "    print('')\n",
    "\n",
    "\n",
    "def match_interest_to_articles(user_interest: str):\n",
    "    global conn, cur\n",
    "    logging.info(f'Looking up the articles for user interest {user_interest}')\n",
    "    logging.info(\"pattern matching the interest to ensure correct range of dates\")\n",
    "    result_pattern = re.findall(date_pattern, user_interest)\n",
    "    if len(result_pattern) == 0:\n",
    "        logging.info(\"Discarding invalid Interest for article date search - does not conform to standards\")\n",
    "        logging.info(f\"INV_I << {user_interest}\")\n",
    "        return None\n",
    "    else:\n",
    "        logging.info(\"Interest matches the expected lookup pattern - Isolating the date\")\n",
    "        result_pattern_tokens = result_pattern[0].split('/')\n",
    "        year = result_pattern_tokens[2]\n",
    "        month = result_pattern_tokens[3]\n",
    "        day = result_pattern_tokens[4]\n",
    "        logging.info(f\"Date extracted: {year}-{month}-{day} - Querying articles for that time\")\n",
    "        result_query = cur.execute(f\"SELECT * FROM news_archive na WHERE article_date = \"\n",
    "                                   f\"DATE(\\'{year}-{month}-{day}\\');\")\n",
    "        result_string = ''\n",
    "        item_num = 0\n",
    "        for result in result_query.fetchall():\n",
    "            result_string += '======================\\n'\n",
    "            result_string = result_string + f'article_date: {result[0]}\\n'\n",
    "            result_string = result_string + f'link: {result[1]}\\n'\n",
    "            result_string = result_string + f'headline: {result[2]}\\n'\n",
    "            result_string = result_string + f'category: {result[3]}\\n'\n",
    "            result_string = result_string + f'authors: {result[4]}\\n'\n",
    "            result_string = result_string + f'short_description: {result[5]}\\n'\n",
    "            result_string += '======================\\n'\n",
    "            item_num += 1\n",
    "        if item_num:\n",
    "            logging.info(f'Found {item_num+1} news items! Sending back to client')\n",
    "            return result_string\n",
    "        else:\n",
    "            logging.info(\"No news items were found for that date - returning None\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def setup_news_table():\n",
    "    global conn, cur\n",
    "    try:\n",
    "        logging.info(\"Setting up the SQLite Database\")\n",
    "        conn = sqlite3.connect(\"news_archive.db\")\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"DROP TABLE IF EXISTS news_archive\")\n",
    "        cur.execute(\"CREATE TABLE news_archive(article_date, link, headline, category, authors, short_description)\")\n",
    "        cur.execute(\"CREATE INDEX article_date_index ON news_archive(article_date) \")\n",
    "        # TODO: offer search by author by indexing the author names.\n",
    "        # cur.execute(\"CREATE INDEX article_authors_index ON news_archive(authors) \")\n",
    "        logging.info(\"Successfully created the table for news_archive in our SQLITE3 Database\")\n",
    "    except Exception:\n",
    "        logging.error(\"Encountered Exception while building the database.\")\n",
    "        logging.error(sys.exc_info())\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    global conn, cur\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        news_list = []\n",
    "        logging.info(\"Reading the news dataset\")\n",
    "        with open('News_Category_Dataset_v3.json', 'r') as news_file:\n",
    "            logging.info(\"Reading the file \\'News_Category_Dataset_v3.json\\'\")\n",
    "            for line in news_file.readlines():\n",
    "                news_item = json.loads(line)\n",
    "                news_list.append((datetime.strptime(news_item['date'], '%Y-%m-%d').date(), news_item['link'],\n",
    "                                  news_item['headline'], news_item['category'], news_item['authors'],\n",
    "                                  news_item['short_description']))\n",
    "            cur.executemany(\"INSERT INTO news_archive VALUES (?, ? , ?, ?, ?, ?)\", news_list)\n",
    "            conn.commit()\n",
    "            logging.info(\"Successfully finished reading the news data into the sqlite database!\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Could not find file for the news dataset from kaggle - please download and unzip the file as\\n\"\n",
    "                      \"\\'News_Category_Dataset_v3.json\\' in the same directory as this program. available at:\\n\"\n",
    "                      \"https://www.kaggle.com/datasets/rmisra/news-category-dataset\")\n",
    "        logging.error(sys.exc_info())\n",
    "        exit(1)\n",
    "    except sqlite3.OperationalError:\n",
    "        logging.error(\"Encountered error while feeding news articles into Sqlite3 DB\")\n",
    "        logging.error(sys.exc_info())\n",
    "        exit()\n",
    "    except Exception as err:\n",
    "        logging.error(\"Suffered from error - while reading file. Cannot Continue\")\n",
    "        logging.debug(sys.exc_info())\n",
    "        exit()\n",
    "    finally:\n",
    "        news_file.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    setup_news_table()\n",
    "    read_dataset()\n",
    "    app.run_forever()\n",
    "\n",
    "```\n",
    "WE start the program with `python3 huffpost_producer.py` and see the following output:\n",
    "```bash\n",
    "# output on the console\n",
    "root@e6ca51d6f22a:~# python3 huffpost_producer.py \n",
    "[2023-05-06 05:01:51]INFO:Setting up the SQLite Database\n",
    "[2023-05-06 05:01:51]INFO:Successfully created the table for news_archive in our SQLITE3 Database\n",
    "[2023-05-06 05:01:51]INFO:Reading the news dataset\n",
    "[2023-05-06 05:01:51]INFO:Reading the file 'News_Category_Dataset_v3.json'\n",
    "[2023-05-06 05:01:55]INFO:Successfully finished reading the news data into the sqlite database!\n",
    "\n",
    "#output of the NFD process on Node 1 -registering the route. \n",
    "1683349315.572415  INFO: [nfd.FaceTable] Added face id=259 remote=fd://18 local=unix:///run/ndn/nfd.sock\n",
    "1683349315.573489  INFO: [nfd.RibManager] Adding route /huffpost/archives nexthop=259 origin=app cost=0\n",
    "```\n",
    "\n",
    "The complimentary consumer program is shown below - it is not much different form the sample program except for the fact that is taking the date from the user as an cli argument in the `YYYY-MM-DD` format and sending that interest to the consumer. At first we send the input `python3 huffpost_consumer.py`\n",
    "\n",
    "```bash\n",
    "import logging\n",
    "import sys\n",
    "import re\n",
    "from ndn import utils, appv2, types\n",
    "from ndn import encoding as enc\n",
    "\n",
    "\n",
    "logging.basicConfig(format='[{asctime}]{levelname}:{message}',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    style='{')\n",
    "\n",
    "# accept and check the cli args\n",
    "date_pattern = '^[0-9]{4}\\\\-[0-9]{1,2}\\\\-[0-9]{1,2}$$'\n",
    "if len(sys.argv) != 2:\n",
    "    print('Insufficient arguments - please run the program with the date in the following format: '\n",
    "          '\\'python huffpost_consumer.py YYYY-MM-DD\\'')\n",
    "    sys.exit(-1)\n",
    "\n",
    "requester_date = sys.argv[1]\n",
    "check_date = re.findall(date_pattern, requester_date)\n",
    "if len(check_date) == 0:\n",
    "    print('Insufficient arguments - please run the program with the date in the following format: '\n",
    "          '\\'python huffpost_consumer.py YYYY-MM-DD\\'')\n",
    "    sys.exit(-1)\n",
    "\n",
    "interest_date = requester_date.replace('-', '/')\n",
    "# initialize the NDNApp\n",
    "\n",
    "app = appv2.NDNApp()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    global requester_date\n",
    "    try:\n",
    "        name = enc.Name.from_str(f'/huffpost/archives/{interest_date}')\n",
    "        print(f'Sending Interest {enc.Name.to_str(name)}, {enc.InterestParam(must_be_fresh=True, lifetime=60000)}')\n",
    "        # TODO: Write a better validator\n",
    "        data_name, content, pkt_context = await app.express(\n",
    "            name, validator=appv2.pass_all,\n",
    "            must_be_fresh=True, can_be_prefix=False, lifetime=60000, no_signature=True)\n",
    "\n",
    "        print(f'Received Data Name: {enc.Name.to_str(data_name)}')\n",
    "        print(pkt_context['meta_info'])\n",
    "        print(bytes(content).decode() if content else None)\n",
    "    except types.InterestNack as e:\n",
    "        print(f'Nacked with reason={e.reason}')\n",
    "    except types.InterestTimeout:\n",
    "        print(f'Timeout')\n",
    "    except types.InterestCanceled:\n",
    "        print(f'Canceled')\n",
    "    except types.ValidationFailure:\n",
    "        print(f'Data failed to validate')\n",
    "    finally:\n",
    "        app.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_forever(after_start=main())\n",
    "```\n",
    "\n",
    "We run the `python3 huffpost_consumer.py 2021-02-04` and see that we get the articles as outputs. The images for those outputs are shown below:\n",
    "\n",
    "![](./images/huffpost-consumer_article_1.png)\n",
    "\n",
    "*Consumer showing article for 2021-20-04*\n",
    "\n",
    "![](./images/huffpost-producer_article_1.png)\n",
    "\n",
    "*Producer showing article for 2021-20-04*\n",
    "\n",
    "![](./images/huffpost-consumer_article_2.png)\n",
    "\n",
    "*Producer showing article for 2021-20-04*\n",
    "\n",
    "![](./images/huffpost-producer_article_2.png)\n",
    "\n",
    "*Producer showing article for 2021-20-04)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62377611-a37b-42a0-a8e8-b8d49d704f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "\n",
    "#Set end host to now plus {num_days} days\n",
    "num_days = 14\n",
    "end_date = (datetime.now(timezone.utc) + timedelta(days=num_days)).strftime(\"%Y-%m-%d %H:%M:%S %z\")\n",
    "\n",
    "try:\n",
    "    slice = fablib.get_slice(name=SLICENAME)\n",
    "\n",
    "    slice.renew(end_date)\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08708fb-2b6d-4988-aac4-c7512c30f6ee",
   "metadata": {},
   "source": [
    "#### Check the new slice end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5051f-3300-4298-928d-2eba8b5fba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    slice = fablib.get_slice(name=SLICENAME)\n",
    "    print(f\"Lease End (UTC)        : {slice.get_lease_end()}\")\n",
    "       \n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351435d-8699-46f3-b5a4-40f457eeb97b",
   "metadata": {},
   "source": [
    "### End Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf094c-3635-407b-83cf-7b60562745d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_list = fablib.get_slices()\n",
    "if slice_list is not []:\n",
    "    print(f\"You have {len(slice_list)} slices\")\n",
    "    for slice in slice_list:\n",
    "        slice.show(output='dict')\n",
    "        # print(f\"deleting {slice}\")\n",
    "        # fablib.delete_slice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515ff76-085a-46c0-9f26-2c441da20ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fablib.delete_slice(SLICENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46117a8-7f85-45d5-8994-4390843dd6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
